{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install python-dateutil opencage nltk ipyleaflet -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import dateutil\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import pipeline, RobertaForTokenClassification, RobertaTokenizerFast, Trainer, TrainingArguments\n",
    "from opencage.geocoder import OpenCageGeocode\n",
    "\n",
    "BUCKET_NAME = <update>\n",
    "INFERENCE_URL = <update>\n",
    "OPENCAGE_API_KEY = <update>\n",
    "THRESHOLD = 0.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conver NER format to JSON format\n",
    "\n",
    "unique_labels = ['O', 'DATE', 'LOCATION']  # add all your labels here\n",
    "label_dict = {label: i for i, label in enumerate(unique_labels)}\n",
    "STOP_WORDS = stopwords.words(\"english\")\n",
    "\n",
    "model = RobertaForTokenClassification.from_pretrained(\n",
    "    'finetuned_roberta_ner',\n",
    "    num_labels=len(unique_labels) # This should match your total number of NER tags\n",
    ")\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('finetuned_roberta_ner', add_prefix_space=True)\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def ner_to_dict(query):\n",
    "    ner_result = ner_pipeline(query)\n",
    "    result = dict()\n",
    "    for item in ner_result:\n",
    "        if item['entity'] in [\"LABEL_1\", \"LABEL_2\"]:\n",
    "            if item['entity'] in result.keys():\n",
    "                result[item['entity']] += item['word']\n",
    "            else:\n",
    "                result[item['entity']] = \"\"\n",
    "                result[item['entity']] += item['word']\n",
    "    old_keys = list(result.keys())\n",
    "    \n",
    "    for key in old_keys:\n",
    "        new_key = unique_labels[int(key[-1])]\n",
    "        result[new_key] = result[key]\n",
    "        del result[key]\n",
    "    \n",
    "    for key, val in result.items():\n",
    "        result[key] = val.replace('Ä ', ' ')[1:]\n",
    "        result[key] = ' '.join([word for word in result[key].split() if word not in STOP_WORDS])\n",
    "        print(key, result[key])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "# Load your domain-specific encoder model (replace 'model_name' with your model's name)\n",
    "model_name = 'event_classifier'\n",
    "classifier_tokenizer = AutoTokenizer.from_pretrained('event_tokenizer')\n",
    "classifier_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4) # 3 event types + 1 'None' class\n",
    "\n",
    "classifier_pipeline = pipeline(\"text-classification\", model=classifier_model, tokenizer=classifier_tokenizer)\n",
    "CLASSES = ['burn_scars', 'crops', 'flood', None]\n",
    "\n",
    "def classifier_to_dict(query):\n",
    "    classifier_result = classifier_pipeline(query)\n",
    "    class_index = int(classifier_result[0]['label'].split('LABEL_')[-1])\n",
    "    return { 'event': CLASSES[class_index] }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier = <update>\n",
    "\n",
    "config_filename = 'burn_scars_Prithvi_100M.py'\n",
    "new_config_filename = f\"configs/{identifier}-{config_filename}\"\n",
    "MODEL_NAME = f\"{identifier}-workshop.pth\"\n",
    "\n",
    "# Check files in the s3 bucket.\n",
    "def check_files():\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(BUCKET_NAME)\n",
    "    for obj in bucket.objects.all():\n",
    "        string_key = str(obj.key)\n",
    "        if new_config_filename in string_key:\n",
    "            config_filename = f\"s3://{BUCKET_NAME}/{obj.key}\"\n",
    "        elif MODEL_NAME in string_key:\n",
    "            model_filename = f\"s3://{BUCKET_NAME}/{obj.key}\" \n",
    "    return { 'config': config_filename, 'model': model_filename }\n",
    "\n",
    "uploaded_files = check_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def geocode(location: str) -> str:\n",
    "    \"\"\"Geocode a query (location, region, or landmark)\"\"\"\n",
    "    opencage_geocoder = OpenCageGeocode(OPENCAGE_API_KEY)\n",
    "    response = opencage_geocoder.geocode(location, no_annotations=\"1\")\n",
    "    if response:\n",
    "        bounds = response[0][\"geometry\"]\n",
    "\n",
    "        # convert to bbox\n",
    "        return [\n",
    "            bounds[\"lng\"] - THRESHOLD,\n",
    "            bounds[\"lat\"] - THRESHOLD,\n",
    "            bounds[\"lng\"] + THRESHOLD,\n",
    "            bounds[\"lat\"] + THRESHOLD,\n",
    "        ]\n",
    "\n",
    "\n",
    "def infer(query):\n",
    "    result = ner_to_dict(query)\n",
    "    result['event'] = classifier_to_dict(query)\n",
    "\n",
    "    if not(result.get('event')) or not(result.get('LOCATION') or not(result.get('DATE'))):\n",
    "        print(\"Please provide valid location name, date, or event type\")\n",
    "        return\n",
    "    bounding_box = geocode(result['LOCATION'])\n",
    "    print(result)\n",
    "    date_str = dateutil.parser.parse(result['DATE']).strftime('%Y-%m-%d')\n",
    "\n",
    "    payload = json.dumps({\n",
    "        \"config_path\": uploaded_files['config'],\n",
    "        \"model_path\": uploaded_files['model'],\n",
    "        \"model_type\": result['event'],\n",
    "        \"date\": date_str,\n",
    "        \"bounding_box\": bounding_box\n",
    "    })\n",
    "    print(payload)\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    # Use deployed app to get inference on the selected date/location\n",
    "    response = requests.request(\n",
    "        \"POST\", \n",
    "        f\"{INFERENCE_URL}/infer\", \n",
    "        headers=headers, \n",
    "        data=payload\n",
    "    )\n",
    "\n",
    "    return {'predictions': response.json(), 'bbox': bounding_box, 'date': date_str, 'event': result['event'], 'location': result['LOCATION']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Show me burn scars in maui from august 13, 2023\"\n",
    "predictions = infer(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ipyleaflet import Map, TileLayer, DrawControl, GeoJSON\n",
    "\n",
    "datestring = predictions['date']\n",
    "HLSL30_TILE_LAYER = 'https://gitc-a.earthdata.nasa.gov/wmts/epsg3857/best/wmts.cgi?TIME=' + datestring + '&layer=HLS_L30_Nadir_BRDF_Adjusted_Reflectance&style=default&tilematrixset=GoogleMapsCompatible_Level12&Service=WMTS&Request=GetTile&Version=1.0.0&Format=image%2Fpng&TileMatrix={z}&TileCol={x}&TileRow={y}'\n",
    "HLSS30_TILE_LAYER = 'https://gitc-a.earthdata.nasa.gov/wmts/epsg3857/best/wmts.cgi?TIME=' + datestring + '&layer=HLS_S30_Nadir_BRDF_Adjusted_Reflectance&style=default&tilematrixset=GoogleMapsCompatible_Level12&Service=WMTS&Request=GetTile&Version=1.0.0&Format=image%2Fpng&TileMatrix={z}&TileCol={x}&TileRow={y}'\n",
    "\n",
    "hlsl30_tile_layer = TileLayer(url=HLSL30_TILE_LAYER, name='HLSL30', attribution='NASA')\n",
    "hlss30_tile_layer = TileLayer(url=HLSS30_TILE_LAYER, name='HLSL30', attribution='NASA')\n",
    "\n",
    "geojson = predictions['predictions']['predictions']\n",
    "\n",
    "detection_map = Map(\n",
    "        center=(\n",
    "            (predictions['bbox'][1] + predictions['bbox'][3]) / 2,\n",
    "            (predictions['bbox'][0] + predictions['bbox'][2]) / 2,\n",
    "        ),\n",
    "        zoom=11, \n",
    "    )\n",
    "detection_map.add(hlsl30_tile_layer)\n",
    "detection_map.add(hlss30_tile_layer)\n",
    "detection_map.add(GeoJSON(data=geojson))\n",
    "\n",
    "detection_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8987afb44532b2110e1a5e1b229dd281f8440b44477d285826a54acdd52d8797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
